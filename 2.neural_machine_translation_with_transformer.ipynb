{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r85Egg5LCr_L"
      },
      "source": [
        "# English-to-Cameroon pidgin english translation with a sequence-to-sequence Transformer\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fotie_codes)<br>\n",
        "**Description:** Implementing a sequence-to-sequence Transformer and training it on a machine translation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9U6DYEjCr_P"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
        "we'll train on an English-to-Spanish machine translation task.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Vectorize text using the Keras `TextVectorization` layer.\n",
        "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
        "and a `PositionalEmbedding` layer.\n",
        "- Prepare data for training a sequence-to-sequence model.\n",
        "- Use the trained model to generate translations of never-seen-before\n",
        "input sentences (sequence-to-sequence inference).\n",
        "\n",
        "The code featured here is adapted from the book\n",
        "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
        "(chapter 11: Deep learning for text).\n",
        "The present example is fairly barebones, so for detailed explanations of\n",
        "how each building block works, as well as the theory behind Transformers,\n",
        "I recommend reading the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fTHbZ1WCr_P"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "88vDxcRsCr_Q"
      },
      "outputs": [],
      "source": [
        "# We set the backend to TensorFlow. The code works with\n",
        "# both `tensorflow` and `torch`. It does not work with JAX\n",
        "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
        "# (used in `TransformerDecoder.get_causal_attention_mask()`:\n",
        "# `tile` in JAX does not support a dynamic `reps` argument.\n",
        "# You can make the code work in JAX by wrapping the\n",
        "# inside of the `get_causal_attention_mask` method in\n",
        "# a decorator to prevent jit compilation:\n",
        "# `with jax.ensure_compile_time_eval():`.\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "\n",
        "import keras as keras\n",
        "from keras import layers\n",
        "#from keras import ops\n",
        "from keras.layers import TextVectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6dfGsXrCr_R"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "og1saQ0bCr_S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_file = \"datasets/english_french.csv\"\n",
        "df = pd.read_csv(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  English      French\n",
            "0     Go.        Va !\n",
            "1     Go.     Marche.\n",
            "2     Go.  En route !\n",
            "3     Go.     Bouge !\n",
            "4     Hi.     Salut !\n"
          ]
        }
      ],
      "source": [
        "print (df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPXXG4GJCr_S"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tNDAwXtgCr_T"
      },
      "outputs": [],
      "source": [
        "text_pairs = []\n",
        "for index, row in df.iterrows():\n",
        "    eng, fre = row['English'], row['French']\n",
        "    fre = \"[start] \" + fre + \" [end]\"\n",
        "    text_pairs.append((eng, fre))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr62rC_ACr_T"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g6xJqV4SCr_U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"What I need isn't money, but your advice.\", \"[start] Ce dont j'ai besoin, ce n'est pas d'argent, mais de tes conseils. [end]\")\n",
            "(\"I've already made it.\", \"[start] Je l'ai déjà fait. [end]\")\n",
            "('We absolutely have to go.', '[start] Nous devons absolument y aller. [end]')\n",
            "(\"I can't give up without a fight.\", '[start] Je ne peux capituler sans combat. [end]')\n",
            "('He was declared bankrupt.', '[start] Il fut déclaré en faillite. [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8O053clCr_U"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6c6uQ3YjCr_U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "229803 total pairs\n",
            "160863 training pairs\n",
            "34470 validation pairs\n",
            "34470 test pairs\n"
          ]
        }
      ],
      "source": [
        "# Shuffle the text pairs to ensure randomness\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "# Calculate the number of validation samples as 15% of the total number of text pairs\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "\n",
        "# Calculate the number of training samples as the total number of text pairs minus twice the number of validation samples\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "# Split the text pairs into training, validation, and test sets\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "# Print the number of total, training, validation, and test pairs\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxpbSKEaCr_V"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
        "data (one for English and one for Spanish),\n",
        "that is to say, to turn the original strings into integer sequences\n",
        "where each integer represents the index of a word in a vocabulary.\n",
        "\n",
        "The English layer will use the default string standardization (strip punctuation characters)\n",
        "and splitting scheme (split on whitespace), while\n",
        "the Spanish layer will use a custom standardization, where we add the character\n",
        "`\"¿\"` to the set of punctuation characters to be stripped.\n",
        "\n",
        "Note: in a production-grade machine translation model, I would not recommend\n",
        "stripping the punctuation characters in either language. Instead, I would recommend turning\n",
        "each punctuation character into its own token,\n",
        "which you could achieve by providing a custom `split` function to the `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-EMKNjVzCr_V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 23:04:36.232597: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-01-08 23:04:36.232716: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# Define a string of characters to be stripped from the input, including all punctuation and the \"¿\" character\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "# Remove the \"[\" and \"]\" characters from the strip_chars string\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# Define the vocabulary size, sequence length, and batch size\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "# Define a function to standardize the input string\n",
        "def custom_standardization(input_string):\n",
        "    # Convert the input string to lowercase\n",
        "    lowercase = tf_strings.lower(input_string)\n",
        "    # Replace all characters in the strip_chars string with an empty string\n",
        "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "# Define a TextVectorization layer for the English texts\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Define a TextVectorization layer for the Spanish texts, with custom standardization\n",
        "fre_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# Extract the English and Spanish texts from the training pairs\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "# Adapt the TextVectorization layers to the training texts\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "fre_vectorization.adapt(train_spa_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIDRAVBBCr_V"
      },
      "source": [
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zI3LvjUkCr_V"
      },
      "outputs": [],
      "source": [
        "# Define a function to format the dataset\n",
        "def format_dataset(eng, spa):\n",
        "    # Vectorize the English and Spanish texts\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = fre_vectorization(spa)\n",
        "    # Return a dictionary with the encoder inputs and decoder inputs, and the target sequences\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "# Define a function to create a dataset from a list of pairs\n",
        "def make_dataset(pairs):\n",
        "    # Unzip the pairs into two lists of English and Spanish texts\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    # Convert the lists to tensors\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    # Create a dataset from the tensors\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    # Batch the dataset\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    # Map the format_dataset function to the dataset\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    # Cache, shuffle, and prefetch the dataset\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "# Create datasets for the training and validation pairs\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSKk5Oa2Cr_W"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 20 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ur1M_MbCr_W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 23:04:37.902148: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 23:04:38.698464: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N735EG3sCr_W"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
        "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
        "we also use a `PositionalEmbedding` layer.\n",
        "\n",
        "The source sequence will be pass to the `TransformerEncoder`,\n",
        "which will produce a new representation of it.\n",
        "This new representation will then be passed\n",
        "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
        "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking\n",
        "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
        "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
        "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
        "(otherwise, it could use information from the future, which would\n",
        "result in a model that cannot be used at inference time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AmsO6C0rCr_W"
      },
      "outputs": [],
      "source": [
        "import keras.ops as ops\n",
        "\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"dense_dim\": self.dense_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, length, 1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if mask is None:\n",
        "            return None\n",
        "        else:\n",
        "            return ops.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
        "            padding_mask = ops.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = ops.arange(sequence_length)[:, None]\n",
        "        j = ops.arange(sequence_length)\n",
        "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
        "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = ops.concatenate(\n",
        "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
        "            axis=0,\n",
        "        )\n",
        "        return ops.tile(mask, mult)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"latent_dim\": self.latent_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM_Y2TdnCr_X"
      },
      "source": [
        "Next, we assemble the end-to-end model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KzKWPU-tCr_X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/fotiem.constant/anaconda3/envs/ml-env/lib/python3.10/site-packages/keras/src/layers/layer.py:1225: UserWarning: Layer 'transformer_encoder' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encoutered: ''No registered 'AssignVariableOp' OpKernel for 'GPU' devices compatible with node {{node AssignVariableOp}}\n",
            "\t (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_UINT32, validate_shape=false\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 16005131165644881776, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='GPU'; dtype in [DT_FLOAT]\n",
            "  device='GPU'; dtype in [DT_INT64]\n",
            "  device='GPU'; dtype in [DT_UINT64]\n",
            "  device='GPU'; dtype in [DT_BOOL]\n",
            "  device='GPU'; dtype in [DT_HALF]\n",
            "  device='GPU'; dtype in [DT_DOUBLE]\n",
            "  device='DEFAULT'; dtype in [DT_VARIANT]\n",
            "  device='CPU'; dtype in [DT_UINT64]\n",
            "  device='CPU'; dtype in [DT_INT64]\n",
            "  device='CPU'; dtype in [DT_UINT32]\n",
            "  device='CPU'; dtype in [DT_UINT16]\n",
            "  device='CPU'; dtype in [DT_INT16]\n",
            "  device='CPU'; dtype in [DT_UINT8]\n",
            "  device='CPU'; dtype in [DT_INT8]\n",
            "  device='CPU'; dtype in [DT_INT32]\n",
            "  device='CPU'; dtype in [DT_HALF]\n",
            "  device='CPU'; dtype in [DT_BFLOAT16]\n",
            "  device='CPU'; dtype in [DT_FLOAT]\n",
            "  device='CPU'; dtype in [DT_DOUBLE]\n",
            "  device='CPU'; dtype in [DT_COMPLEX64]\n",
            "  device='CPU'; dtype in [DT_COMPLEX128]\n",
            "  device='CPU'; dtype in [DT_BOOL]\n",
            "  device='CPU'; dtype in [DT_STRING]\n",
            "  device='CPU'; dtype in [DT_RESOURCE]\n",
            "  device='CPU'; dtype in [DT_VARIANT]\n",
            "  device='CPU'; dtype in [DT_QINT8]\n",
            "  device='CPU'; dtype in [DT_QUINT8]\n",
            "  device='CPU'; dtype in [DT_QINT32]\n",
            "  device='XLA_CPU'\n",
            " [Op:AssignVariableOp]''\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nVariable transformer_encoder/multi_head_attention/query/kernel is already initialized.\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, None, 256), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs={'mask': 'None'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n\u001b[0;32m----> 7\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m encoder \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(encoder_inputs, encoder_outputs)\n\u001b[1;32m     10\u001b[0m decoder_inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/ml-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/ml-env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:37\u001b[0m, in \u001b[0;36mVariable._deferred_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deferred_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m in_stateless_scope():\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are attempting to initialize a variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile in a stateless scope. This is disallowed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that all variables are initialized \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore you start using your layer/model objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nVariable transformer_encoder/multi_head_attention/query/kernel is already initialized.\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, None, 256), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(decoder, to_file='decoder_model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkOriHLECr_X"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eLpYYBlCr_X"
      },
      "outputs": [],
      "source": [
        "epochs = 1  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "history = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot accuracy and all the rest so we see if the model is learning or not\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57khBjPTCr_X"
      },
      "source": [
        "## Decoding test sentences\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the vectorized English sentence\n",
        "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
        "we hit the token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lkFYM_gCr_X"
      },
      "outputs": [],
      "source": [
        "fre_vocab = fre_vectorization.get_vocabulary()\n",
        "fre_index_lookup = dict(zip(range(len(fre_vocab)), fre_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = fre_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n",
        "        sampled_token_index = ops.convert_to_numpy(\n",
        "            ops.argmax(predictions[0, i, :])\n",
        "        ).item(0)\n",
        "        sampled_token = fre_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(f\"Input sentence: {input_sentence}\")\n",
        "    print(f\"Translated:     {translated}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neural_machine_translation_with_transformer",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
